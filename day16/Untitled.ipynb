{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ensemble Learning\n",
    "\n",
    "> \"একতাই শক্তি\" \n",
    "\n",
    "This old saying expresses pretty well the underlying idea that rules the very powerful “ensemble methods” in machine learning. \n",
    "\n",
    "Ensemble learning is a machine learning paradigm where multiple models (often called **“weak learners”**) are trained to solve the same problem and combined to get better results. The main hypothesis is that when weak models are correctly combined we can obtain more accurate and/or robust models.\n",
    "\n",
    "Roughly, ensemble learning methods, that often trust the top rankings of many machine learning competitions (including Kaggle’s competitions), are based on the hypothesis that combining multiple models together can often produce a much more powerful model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Different types of Ensemble Learning techniques\n",
    "\n",
    "There are simple and advanced ensemble learning techniques.\n",
    "\n",
    "**1. Simple:**\n",
    "    1. Max Voting\n",
    "    2. Averaging\n",
    "    3. Weighted Averaging\n",
    "**2. Advanced:**\n",
    "    1. Bagging\n",
    "    2. Boosting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Synthetic Dataset Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1000, 20) (1000, 20)\n"
     ]
    }
   ],
   "source": [
    "# dataset\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.datasets import make_regression\n",
    "\n",
    "\n",
    "# define dataset\n",
    "reg_X, reg_y = make_regression(n_samples=1000, n_features=20, n_informative=12, \n",
    "                               noise=0.1, random_state=5)\n",
    "\n",
    "# define dataset\n",
    "cls_X, cls_y = make_classification(n_samples=1000, n_features=20, n_informative=12, \n",
    "                                   n_classes=5, n_redundant=6, random_state=5)\n",
    "# summarize the dataset\n",
    "print(reg_X.shape, cls_X.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train & test\n",
    "x_train = cls_X[:700]\n",
    "y_train = cls_y[:700]\n",
    "\n",
    "x_test = cls_X[700:]\n",
    "y_test = cls_y[700:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Max Voting\n",
    "\n",
    "The max voting method is generally used for classification problems. In this technique, multiple models are used to make predictions for each data point. The predictions by each model are considered as a `vote`. The predictions which we get from the majority of the models are used as the final prediction.\n",
    "\n",
    "<img src='img/majority_voting.png' width='400'>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "result : \n",
      "model1 accuracy 0.49333333333333335\n",
      "model2 accuracy 0.6933333333333334\n",
      "model3 accuracy 0.55\n",
      "Max Voting accuracy 0.6533333333333333\n"
     ]
    }
   ],
   "source": [
    "from scipy.stats import mode\n",
    "from sklearn.metrics import accuracy_score\n",
    "import numpy as np\n",
    "\n",
    "# For ML\n",
    "from sklearn import tree\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "\n",
    "model1 = tree.DecisionTreeClassifier()\n",
    "model2 = KNeighborsClassifier()\n",
    "model3= LogisticRegression()\n",
    "\n",
    "model1.fit(x_train,y_train)\n",
    "model2.fit(x_train,y_train)\n",
    "model3.fit(x_train,y_train)\n",
    "\n",
    "pred1=model1.predict(x_test)\n",
    "pred2=model2.predict(x_test)\n",
    "pred3=model3.predict(x_test)\n",
    "\n",
    "final_pred = []\n",
    "for i in range(0,len(x_test)):\n",
    "    final_pred.append(mode([pred1[i], pred2[i], pred3[i]])[0])\n",
    "    \n",
    "print(\"result : \")\n",
    "print(f\"model1 accuracy { accuracy_score(y_test, pred1) }\")\n",
    "print(f\"model2 accuracy { accuracy_score(y_test, pred2) }\")\n",
    "print(f\"model3 accuracy { accuracy_score(y_test, pred3) }\")\n",
    "\n",
    "print(f\"Max Voting accuracy { accuracy_score(y_test, final_pred) }\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Averaging\n",
    "Similar to the max voting technique, multiple predictions are made for each data point in averaging. In this method, we take an average of predictions from all the models and use it to make the final prediction. Averaging can be used for making predictions in regression problems or while calculating probabilities for classification problems.\n",
    "\n",
    "<img src='img/average.png' width='400'>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "result : \n",
      "model1 accuracy 0.49333333333333335\n",
      "model2 accuracy 0.6933333333333334\n",
      "model3 accuracy 0.55\n",
      "Avg. accuracy 0.5766666666666667\n"
     ]
    }
   ],
   "source": [
    "model1 = tree.DecisionTreeClassifier()\n",
    "model2 = KNeighborsClassifier()\n",
    "model3= LogisticRegression()\n",
    "\n",
    "model1.fit(x_train,y_train)\n",
    "model2.fit(x_train,y_train)\n",
    "model3.fit(x_train,y_train)\n",
    "\n",
    "pred1=model1.predict_proba(x_test)\n",
    "pred2=model2.predict_proba(x_test)\n",
    "pred3=model3.predict_proba(x_test)\n",
    "\n",
    "finalpred=(pred1+pred2+pred3)/3\n",
    "\n",
    "print(\"result : \")\n",
    "print(f\"model1 accuracy { accuracy_score(y_test, np.argmax(pred1, axis=1)) }\")\n",
    "print(f\"model2 accuracy { accuracy_score(y_test, np.argmax(pred2, axis=1)) }\")\n",
    "print(f\"model3 accuracy { accuracy_score(y_test, np.argmax(pred3, axis=1)) }\")\n",
    "\n",
    "print(f\"Avg. accuracy { accuracy_score(y_test, np.argmax(finalpred, axis=1)) }\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Weighted Average\n",
    "\n",
    "This is an extension of the averaging method. All models are assigned different weights defining the importance of each model for prediction.\n",
    "\n",
    "<img src='img/weighted-unweighted.png' width='450'>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "result : \n",
      "model1 accuracy 0.49\n",
      "model2 accuracy 0.6933333333333334\n",
      "model3 accuracy 0.55\n",
      "w. avg. accuracy 0.69\n"
     ]
    }
   ],
   "source": [
    "model1 = tree.DecisionTreeClassifier()\n",
    "model2 = KNeighborsClassifier()\n",
    "model3= LogisticRegression()\n",
    "\n",
    "model1.fit(x_train,y_train)\n",
    "model2.fit(x_train,y_train)\n",
    "model3.fit(x_train,y_train)\n",
    "\n",
    "pred1=model1.predict_proba(x_test)\n",
    "pred2=model2.predict_proba(x_test)\n",
    "pred3=model3.predict_proba(x_test)\n",
    "\n",
    "finalpred=(pred1*0.2 + pred2*0.5 + pred3*0.3)\n",
    "\n",
    "print(\"result : \")\n",
    "print(f\"model1 accuracy { accuracy_score(y_test, np.argmax(pred1, axis=1)) }\")\n",
    "print(f\"model2 accuracy { accuracy_score(y_test, np.argmax(pred2, axis=1)) }\")\n",
    "print(f\"model3 accuracy { accuracy_score(y_test, np.argmax(pred3, axis=1)) }\")\n",
    "\n",
    "print(f\"w. avg. accuracy { accuracy_score(y_test, np.argmax(finalpred, axis=1)) }\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> # Task #1 (8 mins)\n",
    "\n",
    "1. Make a Synthetic Dataset for classification with n_classes = 4\n",
    "2. Try Max Voting Ensemble Learning techniques on that dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Advanced Ensemble techniques\n",
    "Let’s move on to understanding the advanced techniques\n",
    "\n",
    "1. Bagging\n",
    "2. Boosting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bagging\n",
    "\n",
    "The idea behind bagging is combining the results of multiple models (for instance, all decision trees) to get a generalized result. Here’s a question: If you create all the models on the same set of data and combine it, will it be useful? There is a high chance that these models will give the same result since they are getting the same input. \n",
    "\n",
    "So how can we solve this problem? One of the techniques is bootstrapping.\n",
    "\n",
    "Bootstrapping is a sampling technique in which we create subsets of observations from the original dataset, **with replacement / replica**. \n",
    "\n",
    "<img src='img/bootstrap.png' width='350'>\n",
    "\n",
    "Bagging <  **\"Bootstrap Aggregation\"** \n",
    "\n",
    "\n",
    "<img src='img/bagging1.png' width='650'>\n",
    "\n",
    "\n",
    "**Bagging algorithms:**\n",
    "\n",
    "1. Bagging meta-estimator\n",
    "2. Random forest\n",
    "\n",
    "\n",
    "## Bagging meta-estimator\n",
    "\n",
    "Bagging meta-estimator is an ensembling algorithm that can be used for both classification (BaggingClassifier) and regression (BaggingRegressor) problems. It follows the typical bagging technique to make predictions. Following are the steps for the bagging meta-estimator algorithm:\n",
    "\n",
    "Random subsets are created from the original dataset (Bootstrapping).\n",
    "The subset of the dataset includes all features.\n",
    "A user-specified base estimator is fitted on each of these smaller sets.\n",
    "Predictions from each model are combined to get the final result.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6166666666666667"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.ensemble import BaggingClassifier\n",
    "from sklearn.ensemble import BaggingRegressor\n",
    "from sklearn import tree\n",
    "\n",
    "model = BaggingClassifier(n_estimators=5)\n",
    "\n",
    "model.fit(x_train, y_train)\n",
    "model.score(x_test,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7133333333333334"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# KNN \n",
    "model = BaggingClassifier(base_estimator=KNeighborsClassifier(), \n",
    "                          n_estimators=30)\n",
    "model.fit(x_train, y_train)\n",
    "model.score(x_test,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train & test for regression\n",
    "Rx_train = reg_X[:700]\n",
    "Ry_train = reg_y[:700]\n",
    "\n",
    "Rx_test = reg_X[700:]\n",
    "Ry_test = reg_y[700:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> # Task #2 (5 mins)\n",
    "1. Use Bagging meta-estimator for regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# code here task #2\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Boosting\n",
    "\n",
    "Before we go further, here’s another question for you: If a data point is incorrectly predicted by the first model, and then the next (probably all models), will combining the predictions provide better results? Such situations are taken care of by boosting.\n",
    "\n",
    "Boosting is a sequential process, where each subsequent model attempts to correct the errors of the previous model. The succeeding models are dependent on the previous model. Let’s understand the way boosting works in the below steps.\n",
    "\n",
    "1. First, generate Random Sample from Training Data-set.\n",
    "2. Now, Train a classifier model 1 for this generated sample data and test the whole training data-set.\n",
    "3. Now, Calculate the error for each instance prediction. if the instance is classified wrongly, increase the weight for that instance and create another sample.\n",
    "4. Repeat this process until you get high accuracy from the system.\n",
    "\n",
    "<img src='img/boosting.png' width='550'>\n",
    "\n",
    "Some boosting algorithms: \n",
    "\n",
    "1. XGBoost\n",
    "2. Light GBM\n",
    "3. CatBoost\n",
    "\n",
    "<img src='img/history.png' width='650'>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# XGBoost\n",
    "\n",
    "XGBoost stands for eXtreme Gradient Boosting.\n",
    "\n",
    "XGBoost (extreme Gradient Boosting) is an advanced implementation of the gradient boosting algorithm. XGBoost has proved to be a highly effective ML algorithm, extensively used in machine learning competitions and hackathons. XGBoost has high predictive power and is almost 10 times faster than the other gradient boosting techniques. It also includes a variety of regularization which reduces overfitting and improves overall performance. Hence it is also known as **regularized boosting** technique."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6933333333333334"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from xgboost import XGBClassifier, XGBRegressor\n",
    "\n",
    "model=XGBClassifier()\n",
    "\n",
    "model.fit(x_train, y_train)\n",
    "model.score(x_test,y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> # Task #3 (5 mins)\n",
    "\n",
    "Apply XGBRegressor on regression dataset and check score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "# task #3 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Light GBM\n",
    "\n",
    "Before discussing how Light GBM works, let’s first understand why we need this algorithm when we have so many others (like the ones we have seen above). Light GBM beats all the other algorithms when the dataset is extremely large. Compared to the other algorithms, Light GBM takes lesser time to run on a huge dataset.\n",
    "\n",
    "LightGBM is a gradient boosting framework that uses tree-based algorithms and follows leaf-wise approach while other algorithms work in a level-wise approach pattern. The images below will help you understand the difference in a better way.\n",
    "\n",
    "<img src='img/light.png' width='450'>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1]\tvalid_0's multi_logloss: 1.32611\n",
      "[2]\tvalid_0's multi_logloss: 1.27833\n",
      "[3]\tvalid_0's multi_logloss: 1.23469\n",
      "[4]\tvalid_0's multi_logloss: 1.19213\n",
      "[5]\tvalid_0's multi_logloss: 1.15552\n",
      "[6]\tvalid_0's multi_logloss: 1.12095\n",
      "[7]\tvalid_0's multi_logloss: 1.08861\n",
      "[8]\tvalid_0's multi_logloss: 1.05937\n",
      "[9]\tvalid_0's multi_logloss: 1.02986\n",
      "[10]\tvalid_0's multi_logloss: 1.00278\n",
      "[11]\tvalid_0's multi_logloss: 0.97633\n",
      "[12]\tvalid_0's multi_logloss: 0.958384\n",
      "[13]\tvalid_0's multi_logloss: 0.940679\n",
      "[14]\tvalid_0's multi_logloss: 0.92083\n",
      "[15]\tvalid_0's multi_logloss: 0.90462\n",
      "[16]\tvalid_0's multi_logloss: 0.886954\n",
      "[17]\tvalid_0's multi_logloss: 0.870984\n",
      "[18]\tvalid_0's multi_logloss: 0.858184\n",
      "[19]\tvalid_0's multi_logloss: 0.845644\n",
      "[20]\tvalid_0's multi_logloss: 0.833871\n",
      "[21]\tvalid_0's multi_logloss: 0.81878\n",
      "[22]\tvalid_0's multi_logloss: 0.81097\n",
      "[23]\tvalid_0's multi_logloss: 0.801009\n",
      "[24]\tvalid_0's multi_logloss: 0.789756\n",
      "[25]\tvalid_0's multi_logloss: 0.779964\n",
      "[26]\tvalid_0's multi_logloss: 0.770016\n",
      "[27]\tvalid_0's multi_logloss: 0.762471\n",
      "[28]\tvalid_0's multi_logloss: 0.753415\n",
      "[29]\tvalid_0's multi_logloss: 0.747161\n",
      "[30]\tvalid_0's multi_logloss: 0.742413\n",
      "[31]\tvalid_0's multi_logloss: 0.734612\n",
      "[32]\tvalid_0's multi_logloss: 0.728779\n",
      "[33]\tvalid_0's multi_logloss: 0.725744\n",
      "[34]\tvalid_0's multi_logloss: 0.722781\n",
      "[35]\tvalid_0's multi_logloss: 0.720429\n",
      "[36]\tvalid_0's multi_logloss: 0.716588\n",
      "[37]\tvalid_0's multi_logloss: 0.714019\n",
      "[38]\tvalid_0's multi_logloss: 0.71236\n",
      "[39]\tvalid_0's multi_logloss: 0.706437\n",
      "[40]\tvalid_0's multi_logloss: 0.701489\n",
      "[41]\tvalid_0's multi_logloss: 0.699366\n",
      "[42]\tvalid_0's multi_logloss: 0.69653\n",
      "[43]\tvalid_0's multi_logloss: 0.694048\n",
      "[44]\tvalid_0's multi_logloss: 0.689751\n",
      "[45]\tvalid_0's multi_logloss: 0.687025\n",
      "[46]\tvalid_0's multi_logloss: 0.684001\n",
      "[47]\tvalid_0's multi_logloss: 0.68269\n",
      "[48]\tvalid_0's multi_logloss: 0.680844\n",
      "[49]\tvalid_0's multi_logloss: 0.679017\n",
      "[50]\tvalid_0's multi_logloss: 0.679245\n",
      "[51]\tvalid_0's multi_logloss: 0.677299\n",
      "[52]\tvalid_0's multi_logloss: 0.675601\n",
      "[53]\tvalid_0's multi_logloss: 0.67349\n",
      "[54]\tvalid_0's multi_logloss: 0.670949\n",
      "[55]\tvalid_0's multi_logloss: 0.670739\n",
      "[56]\tvalid_0's multi_logloss: 0.671742\n",
      "[57]\tvalid_0's multi_logloss: 0.670946\n",
      "[58]\tvalid_0's multi_logloss: 0.670249\n",
      "[59]\tvalid_0's multi_logloss: 0.667749\n",
      "[60]\tvalid_0's multi_logloss: 0.668514\n",
      "[61]\tvalid_0's multi_logloss: 0.669013\n",
      "[62]\tvalid_0's multi_logloss: 0.668515\n",
      "[63]\tvalid_0's multi_logloss: 0.66729\n",
      "[64]\tvalid_0's multi_logloss: 0.668324\n",
      "[65]\tvalid_0's multi_logloss: 0.667109\n",
      "[66]\tvalid_0's multi_logloss: 0.666691\n",
      "[67]\tvalid_0's multi_logloss: 0.665475\n",
      "[68]\tvalid_0's multi_logloss: 0.6645\n",
      "[69]\tvalid_0's multi_logloss: 0.666874\n",
      "[70]\tvalid_0's multi_logloss: 0.665798\n",
      "[71]\tvalid_0's multi_logloss: 0.664697\n",
      "[72]\tvalid_0's multi_logloss: 0.664413\n",
      "[73]\tvalid_0's multi_logloss: 0.662365\n",
      "[74]\tvalid_0's multi_logloss: 0.660656\n",
      "[75]\tvalid_0's multi_logloss: 0.660237\n",
      "[76]\tvalid_0's multi_logloss: 0.661323\n",
      "[77]\tvalid_0's multi_logloss: 0.662335\n",
      "[78]\tvalid_0's multi_logloss: 0.663491\n",
      "[79]\tvalid_0's multi_logloss: 0.665302\n",
      "[80]\tvalid_0's multi_logloss: 0.664224\n",
      "[81]\tvalid_0's multi_logloss: 0.665063\n",
      "[82]\tvalid_0's multi_logloss: 0.667225\n",
      "[83]\tvalid_0's multi_logloss: 0.667718\n",
      "[84]\tvalid_0's multi_logloss: 0.667766\n",
      "[85]\tvalid_0's multi_logloss: 0.670004\n",
      "[86]\tvalid_0's multi_logloss: 0.667799\n",
      "[87]\tvalid_0's multi_logloss: 0.665882\n",
      "[88]\tvalid_0's multi_logloss: 0.665176\n",
      "[89]\tvalid_0's multi_logloss: 0.664468\n",
      "[90]\tvalid_0's multi_logloss: 0.664903\n",
      "[91]\tvalid_0's multi_logloss: 0.664811\n",
      "[92]\tvalid_0's multi_logloss: 0.665014\n",
      "[93]\tvalid_0's multi_logloss: 0.663766\n",
      "[94]\tvalid_0's multi_logloss: 0.664015\n",
      "[95]\tvalid_0's multi_logloss: 0.665836\n",
      "[96]\tvalid_0's multi_logloss: 0.668444\n",
      "[97]\tvalid_0's multi_logloss: 0.669525\n",
      "[98]\tvalid_0's multi_logloss: 0.669868\n",
      "[99]\tvalid_0's multi_logloss: 0.671556\n",
      "[100]\tvalid_0's multi_logloss: 0.671611\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.76"
      ]
     },
     "execution_count": 141,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from lightgbm import LGBMClassifier, LGBMRegressor\n",
    "\n",
    "model = LGBMClassifier()\n",
    "\n",
    "model.fit(x_train, y_train, eval_set=(x_test, y_test))\n",
    "model.score(x_test,y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> # Task #4 (5 mins)\n",
    "\n",
    "Apply LGBMRegressor on regression dataset and check score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "# task #4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CatBoost\n",
    "\n",
    "`CatBoost` name comes from two words `Category` and `Boosting`.\n",
    "\n",
    "Handling categorical variables is a tedious process, especially when you have a large number of such variables. When your categorical variables have too many labels (i.e. they are highly cardinal), performing one-hot-encoding on them exponentially increases the dimensionality and it becomes really difficult to work with the dataset.\n",
    "\n",
    "CatBoost can automatically deal with categorical variables and does not require extensive data preprocessing like other machine learning algorithms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8033333333333333"
      ]
     },
     "execution_count": 140,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from catboost import CatBoostClassifier\n",
    "model=CatBoostClassifier()\n",
    "model.fit(x_train,y_train,eval_set=(x_test, y_test))\n",
    "model.score(x_test,y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> # Task #5 (5 mins)\n",
    "\n",
    "Apply CatBoostRegressor on regression dataset and check score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'conda' is not recognized as an internal or external command,\n",
      "operable program or batch file.\n"
     ]
    }
   ],
   "source": [
    "# ! pip install lightgbm catboost xgboost "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip install catboost"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
